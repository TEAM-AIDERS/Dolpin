"""
LangGraph ë…¸ë“œ ëž˜í¼ í•¨ìˆ˜
ê° ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•˜ê³  Stateë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

ë²„ì „: v2 (260203)
- state ì €ìž¥ í˜•íƒœ í™•ìž¥ ë°˜ì˜
- node_insights í‚¤ë¥¼ graph node idì™€ ë™ì¼í•˜ê²Œ í†µì¼
- lexicon_lookupëŠ” MCPClient singleton(get_mcp_client) ì‚¬ìš©
"""

import logging, os
from datetime import datetime, timezone
from typing import Dict, Any, Optional

from .state import AnalysisState, ErrorLog
from .edges import (
    route_after_spike_analysis,
    route_after_sentiment,
    route_after_causality
)

from src.agents.sentiment_agent import build_agent as build_sentiment_agent

logger = logging.getLogger(__name__)


# ============================================================
# ê³µí†µ ìœ í‹¸
# ============================================================

def _ensure_state_collections(state: AnalysisState) -> None:
    """stateì— ê¸°ë³¸ ì»¬ë ‰ì…˜ë“¤ì´ ì—†ìœ¼ë©´ ì´ˆê¸°í™”"""
    if "error_logs" not in state or state["error_logs"] is None:
        state["error_logs"] = []
    if "node_insights" not in state or state["node_insights"] is None:
        state["node_insights"] = {}


def _add_error_log(
    state: AnalysisState,
    stage: str,
    error_type: str,
    message: str,
    details: Optional[Dict[str, Any]] = None
) -> None:
    """ì—ëŸ¬ ë¡œê·¸ ì¶”ê°€"""
    _ensure_state_collections(state)

    error_log: ErrorLog = {
        "stage": stage,  # state.py Literalì— ë§žì¶°ì•¼ í•¨
        "error_type": error_type,
        "message": message,
        "occurred_at": datetime.utcnow().isoformat() + "Z",
        "trace_id": state.get("trace_id", "unknown"),
        "details": details
    }
    state["error_logs"].append(error_log)
    logger.error(f"[{stage}] {error_type}: {message}", extra={"trace_id": state.get("trace_id", "unknown")})


def _update_node_insight(state: AnalysisState, node_id: str, insight: str) -> None:
    """node_insights ì—…ë°ì´íŠ¸ (node_idëŠ” graph.add_node ì´ë¦„ê³¼ ë™ì¼í•˜ê²Œ)"""
    _ensure_state_collections(state)
    state["node_insights"][node_id] = insight
    logger.info(f"[{node_id}] {insight}", extra={"trace_id": state.get("trace_id", "unknown")})


def _utcnow_z() -> str:
    return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")



# ============================================================
# spike_analyzer
# ============================================================

def spike_analyzer_node(state: AnalysisState) -> AnalysisState:
    """
    ê¸‰ë“± ë¶„ì„ ë…¸ë“œ (í˜„ìž¬ stub)
    TODO: spike analyzer ì‹¤ì œ ì—°ê²° ì‹œ êµì²´
    """
    _ensure_state_collections(state)

    try:
        # Stub: ë”ë¯¸ ë°ì´í„°
        spike_event = state["spike_event"]
        result = {
            "is_significant": True,
            "spike_rate": 3.5,
            "spike_type": "organic",
            "spike_nature": "positive",
            "peak_timestamp": "2026-01-10T10:30:00Z",
            "duration_minutes": 60,
            "confidence": 0.85,
            "actionability_score": 0.7,
            "data_completeness": "confirmed",
            "partial_data_warning": None,
            "viral_indicators": {
                "is_trending": True,
                "has_breakout": True,
                "max_rise_rate": "Breakout",
                "breakout_queries": ["ë‰´ì§„ìŠ¤ ì»´ë°±"],
                "cross_platform": ["twitter", "google_trends"],
                "international_reach": 0.3
            }
        }

        state["spike_analysis"] = result

        insight = f"{result['spike_rate']}ë°° ê¸‰ë“±, {result['spike_nature']} ë°”ì´ëŸ´"
        if result.get("partial_data_warning"):
            insight += f", {result['partial_data_warning']}"
        _update_node_insight(state, "spike_analyzer", insight)

        logger.info(f"SpikeAnalyzer ì™„ë£Œ: is_significant={result['is_significant']}")
        return state

    except Exception as e:
        _add_error_log(
            state,
            stage="spike_analyzer",
            error_type="exception",
            message=str(e),
            details={"keyword": state.get("spike_event", {}).get("keyword")}
        )

        # ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ê°’ ì„¤ì •
        state["spike_analysis"] = {
            "is_significant": False,
            "spike_rate": 0.0,
            "spike_type": "organic",
            "spike_nature": "neutral",
            "peak_timestamp": _utcnow_z(),
            "duration_minutes": 0,
            "confidence": 0.0,
            "actionability_score": 0.0,
            "data_completeness": "partial",
            "partial_data_warning": "ë¶„ì„ ì‹¤íŒ¨",
            "viral_indicators": {
                "is_trending": False,
                "has_breakout": False,
                "max_rise_rate": "0%",
                "breakout_queries": [],
                "cross_platform": [],
                "international_reach": 0.0
            }
        }
        _update_node_insight(state, "spike_analyzer", "ê¸‰ë“± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        return state


# ============================================================
# router1
# ============================================================

def router1_node(state: AnalysisState) -> AnalysisState:
    """Router 1ì°¨: Skip vs Analyze"""
    _ensure_state_collections(state)

    try:
        decision = route_after_spike_analysis(state)
        state["route1_decision"] = decision

        if decision == "skip":
            state["skipped"] = True
            state["skip_reason"] = "not_significant"
            _update_node_insight(state, "router1", "skip: not_significant")
        else:
            state["skipped"] = False
            state["skip_reason"] = None
            _update_node_insight(state, "router1", "analyze: significant")

        return state

    except Exception as e:
        _add_error_log(state, "spike_analyzer", "exception", f"Router1 ì—ëŸ¬: {str(e)}")
        state["route1_decision"] = "skip"
        state["skipped"] = True
        state["skip_reason"] = "not_significant"
        _update_node_insight(state, "router1", "skip: exception")
        return state


# ============================================================
# lexicon_lookup
# ============================================================

def lexicon_lookup_node(state: AnalysisState) -> AnalysisState:
    """
    ë ‰ì‹œì½˜ ë§¤ì¹­ ë…¸ë“œ
    - MCPClient singleton(get_mcp_client) ì‚¬ìš©
    - state["lexicon_matches"]: Optional[Dict[str, LexiconMatch]]
      LexiconMatch = { "count": int, "type": str, "terms": List[str] }
    """
    _ensure_state_collections(state)

    try:
        spike_event = state.get("spike_event")
        messages = (spike_event or {}).get("messages", [])
        if not messages:
            state["lexicon_matches"] = None
            _update_node_insight(state, "lexicon_lookup", "ë¶„ì„í•  ë©”ì‹œì§€ ì—†ìŒ")
            return state

        # singleton ì‚¬ìš©
        from src.server.mcp_client import get_mcp_client
        lexicon_path = state.get("lexicon_path", "custom_lexicon.csv")
        mcp = get_mcp_client(lexicon_path)

        # type -> LexiconMatch
        lexicon_matches_result: Dict[str, Dict[str, Any]] = {}

        for msg in messages:
            text = (msg or {}).get("text", "")
            if not text:
                continue

            res = mcp.lexicon_analyze(text) or {}
            matches = res.get("matches", [])  # dict ë¦¬í„´ ê¸°ì¤€

            for match in matches:
                type_name = match.get("type", "unknown")
                term = match.get("term")

                if type_name not in lexicon_matches_result:
                    lexicon_matches_result[type_name] = {
                        "count": 0,
                        "type": type_name,
                        "terms": []
                    }

                lexicon_matches_result[type_name]["count"] += 1
                if term and term not in lexicon_matches_result[type_name]["terms"]:
                    lexicon_matches_result[type_name]["terms"].append(term)

        state["lexicon_matches"] = lexicon_matches_result if lexicon_matches_result else None

        if lexicon_matches_result:
            top = sorted(
                lexicon_matches_result.items(),
                key=lambda x: x[1]["count"],
                reverse=True
            )[:3]
            summary = ", ".join([f"{k}({v['count']})" for k, v in top])
            _update_node_insight(state, "lexicon_lookup", f"ë ‰ì‹œì½˜ ë§¤ì¹­: {summary}")
        else:
            _update_node_insight(state, "lexicon_lookup", "ë ‰ì‹œì½˜ ë§¤ì¹­ ê²°ê³¼ ì—†ìŒ")

        logger.info(f"LexiconLookup ì™„ë£Œ: {len(lexicon_matches_result)} íƒ€ìž…")
        return state

    except Exception as e:
        _add_error_log(
            state,
            stage="lexicon_lookup",  
            error_type="exception",
            message=f"LexiconLookup ì‹¤íŒ¨: {str(e)}",
            details={"keyword": state.get("spike_event", {}).get("keyword")}
        )
        state["lexicon_matches"] = None
        _update_node_insight(state, "lexicon_lookup", "ë ‰ì‹œì½˜ ë§¤ì¹­ ì‹¤íŒ¨(ì˜ˆì™¸)")
        return state


# ============================================================
# sentiment
# ============================================================

def sentiment_node(state: AnalysisState) -> AnalysisState:
    """ê°ì • ë¶„ì„ ë…¸ë“œ"""
    _ensure_state_collections(state)

    try:
        spike_event = state.get("spike_event")
        messages = (spike_event or {}).get("messages", [])
        if not messages:
            state["sentiment_result"] = None
            _update_node_insight(state, "sentiment", "ë¶„ì„í•  ë©”ì‹œì§€ ì—†ìŒ")
            return state

        combined_text = " ".join([(m or {}).get("text", "") for m in messages]).strip()
        if not combined_text:
            state["sentiment_result"] = None
            _update_node_insight(state, "sentiment", "í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìžˆìŒ")
            return state

        MODEL_PATH = state.get("sentiment_model_path", "models/sentiment_model")
        LEXICON_PATH = state.get("lexicon_path", "custom_lexicon.csv")
        DEVICE = state.get("device", "cpu")

        try:
            agent = build_sentiment_agent(MODEL_PATH, LEXICON_PATH, DEVICE)
        except Exception as e:
            logger.error(f"SentimentAgent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            state["sentiment_result"] = None
            _update_node_insight(state, "sentiment", "ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return state

        # ê°ì • ë¶„ì„ ì‹¤í–‰
        analyzed_count = len(messages)
        result, route_meta = agent.analyze(combined_text, analyzed_count=analyzed_count)

        # stateì— í•„ìš”í•œ í•„ë“œ ì¶”ê°€
        result["analyzed_count"] = analyzed_count
        result["confidence"] = result.get("confidence", 0.5)
        result["sentiment_shift"] = result.get("sentiment_shift", "stable")

        # representative_messagesê°€ ì´ë¯¸ í¬ë§·íŒ…ë˜ì–´ ìžˆì„ ìˆ˜ë„ ìžˆìœ¼ë‹ˆ ì•ˆì „í•˜ê²Œ
        if "representative_messages" not in result or not result["representative_messages"]:
            dom = result.get("dominant_sentiment", "support")
            result["representative_messages"] = {dom: [combined_text[:200]]}

        state["sentiment_result"] = result

        # Insight ìƒì„±
        dominant = result.get("dominant_sentiment", "unknown")
        confidence = float(result.get("confidence", 0.0))
        _update_node_insight(
            state,
            "sentiment",
            f"{dominant} {confidence*100:.0f}% (ì‹ ë¢°ë„), {analyzed_count}ê±´"
        )

        logger.info(
            f"Sentiment ì™„ë£Œ: dominant={dominant}, confidence={confidence:.2f}, "
            f"route={route_meta.get('route', 'unknown')}"
        )
        return state

    except Exception as e:
        _add_error_log(
            state,
            stage="sentiment",
            error_type="exception",
            message=str(e),
            details={"keyword": state.get("spike_event", {}).get("keyword")}
        )
        state["sentiment_result"] = None
        _update_node_insight(state, "sentiment", "ê°ì • ë¶„ì„ ì‹¤íŒ¨(ì˜ˆì™¸)")
        return state


# ============================================================
# router2
# ============================================================

def router2_node(state: AnalysisState) -> AnalysisState:
    """
    Router 2ì°¨: Sentiment Only vs Full Analysis
    
    Note: positive_viral_detectedë„ ì—¬ê¸°ì„œ ì„¤ì • (Router 3ì°¨ì—ì„œ ìž¬ì‚¬ìš©)
    """
    _ensure_state_collections(state)

    try:
        spike_analysis = state.get("spike_analysis")
        sentiment_result = state.get("sentiment_result")

        if not spike_analysis or not sentiment_result:
            state["route2_decision"] = "sentiment_only"
            state["positive_viral_detected"] = False
            _update_node_insight(state, "router2", "sentiment_only (missing inputs)")
            return state

        # positive_viral_detected ë¨¼ì € ê³„ì‚°
        from .edges import _check_opportunity_signals
        has_opportunity = _check_opportunity_signals(spike_analysis, sentiment_result)
        state["positive_viral_detected"] = has_opportunity

        decision = route_after_sentiment(state)
        state["route2_decision"] = decision

        _update_node_insight(
            state,
            "router2",
            f"{decision}, positive_viral={has_opportunity}, actionability={spike_analysis.get('actionability_score')}"
        )
        return state

    except Exception as e:
        _add_error_log(state, "sentiment", "exception", f"Router2 ì—ëŸ¬: {str(e)}")
        state["route2_decision"] = "sentiment_only"
        state["positive_viral_detected"] = False
        _update_node_insight(state, "router2", "sentiment_only (exception)")
        return state


# ============================================================
# causality
# ============================================================

def causality_node(state: AnalysisState) -> AnalysisState:
    """
    ì¸ê³¼ê´€ê³„ ë¶„ì„ ë…¸ë“œ (í˜„ìž¬ stub)
    """
    _ensure_state_collections(state)

    try:
        result = {
            "trigger_source": "influencer",
            "hub_accounts": [
                {
                    "account_id": "user_abc",
                    "influence_score": 0.9,
                    "follower_count": 150000,
                    "account_type": "influencer"
                }
            ],
            "retweet_network_metrics": {"centralization": 0.7, "avg_degree": 15.3},
            "cascade_pattern": "viral",
            "estimated_origin_time": "2026-01-10T09:30:00Z",
            "key_propagation_paths": ["ì¸í”Œë£¨ì–¸ì„œA â†’ íŒ¬ê³„ì •B â†’ ì¼ë°˜ìœ ì €ë“¤"]
        }

        state["causality_result"] = result
        _update_node_insight(state, "causality", f"{result['trigger_source']} ì£¼ë„, {result['cascade_pattern']} íŒ¨í„´")
        return state

    except Exception as e:
        _add_error_log(state, "causality", "exception", str(e))
        state["causality_result"] = None
        _update_node_insight(state, "causality", "ì¸ê³¼ ë¶„ì„ ì‹¤íŒ¨(ì˜ˆì™¸)")
        return state


# ============================================================
# router3
# ============================================================

def router3_node(state: AnalysisState) -> AnalysisState:
    """Router 3ì°¨: Legal vs Amplification"""
    _ensure_state_collections(state)

    try:
        decision = route_after_causality(state)
        state["route3_decision"] = decision
        _update_node_insight(state, "router3", decision)
        return state

    except Exception as e:
        _add_error_log(state, "causality", "exception", f"Router3 ì—ëŸ¬: {str(e)}")
        state["route3_decision"] = "legal"
        _update_node_insight(state, "router3", "legal (exception)")
        return state


# ============================================================
# legal_rag
# ============================================================

def legal_rag_node(state: AnalysisState) -> AnalysisState:
    """
    ë²•ë¥  ë¦¬ìŠ¤í¬ ê²€í†  ë…¸ë“œ (í˜„ìž¬ stub)
    """
    _ensure_state_collections(state)

    try:
        result = {
            "overall_risk_level": "low",
            "clearance_status": "clear",
            "confidence": 0.95,
            "rag_required": False,
            "rag_performed": False,
            "rag_confidence": None,
            "risk_assessment": None,
            "recommended_action": [],
            "referenced_documents": [],
            "signals": {
                "legal_keywords_detected": False,
                "matched_keywords": [],
                "reason": "none"
            }
        }

        # State ì—…ë°ì´íŠ¸
        state["legal_risk"] = result
        _update_node_insight(state, "legal_rag", f"{result['overall_risk_level']}/{result['clearance_status']}")
        return state

    except Exception as e:
        _add_error_log(state, "legal_rag", "exception", str(e))
        state["legal_risk"] = {
            "overall_risk_level": "medium",
            "clearance_status": "review_needed",
            "confidence": 0.5,
            "rag_required": False,
            "rag_performed": False,
            "rag_confidence": None,
            "risk_assessment": None,
            "recommended_action": ["ìˆ˜ë™ ê²€í†  í•„ìš”"],
            "referenced_documents": [],
            "signals": None
        }
        _update_node_insight(state, "legal_rag", "review_needed (exception)")
        return state


# ============================================================
# amplification
# ============================================================

def amplification_node(state: AnalysisState) -> AnalysisState:
    """ê¸ì • ë°”ì´ëŸ´ ê¸°íšŒ ìš”ì•½ ë…¸ë“œ"""
    _ensure_state_collections(state)
    
    try:
        causality = state.get("causality_result") or {}
        sentiment = state.get("sentiment_result") or {}
        spike = state.get("spike_analysis") or {}
        
        viral_indicators = spike.get("viral_indicators", {})
        
        # ===== Hub Accounts =====
        hub_accounts = []
        for hub in (causality.get("hub_accounts") or [])[:5]:
            hub_accounts.append({
                "account_id": hub["account_id"],
                "influence_score": hub["influence_score"],
                "account_type": hub.get("account_type", "general"),
                "follower_count": hub.get("follower_count", 0)
            })
        
        # ===== Representative Messages (ì´ˆê°„ë‹¨!) =====
        support_msgs = (sentiment.get("representative_messages", {}).get("support") or [])
        representative_messages = [
            {"text": msg}  # ðŸ‘ˆ textë§Œ!
            for msg in support_msgs[:5]
        ]
        
        # ===== ê²°ê³¼ =====
        result = {
            "top_platforms": viral_indicators.get("cross_platform", ["twitter"]),
            "hub_accounts": hub_accounts,
            "representative_messages": representative_messages
        }
        
        state["amplification_summary"] = result
        
        platform_count = len(result["top_platforms"])
        hub_count = len(result["hub_accounts"])
        msg_count = len(result["representative_messages"])
        _update_node_insight(
            state, 
            "amplification", 
            f"{platform_count}ê°œ í”Œëž«í¼, {hub_count}ê°œ í—ˆë¸Œ, {msg_count}ê°œ ë©”ì‹œì§€"
        )
        
        return state
    
    except Exception as e:
        _add_error_log(state, "amplification", "exception", str(e))
        state["amplification_summary"] = None
        _update_node_insight(state, "amplification", "ì‹¤íŒ¨")
        return state


# ============================================================
# exec_brief
# ============================================================

def exec_brief_node(state: AnalysisState) -> AnalysisState:
    """ë¸Œë¦¬í•‘ ìƒì„± ë…¸ë“œ"""
    _ensure_state_collections(state)

    try:
        workflow_start = datetime.fromisoformat(state["workflow_start_time"].replace("Z", "+00:00"))
        duration = (datetime.now(workflow_start.tzinfo) - workflow_start).total_seconds()

        # ê° ì„¹ì…˜ ìš”ì•½ ìƒì„±
        spike_summary = _generate_spike_summary(state)
        sentiment_summary = _generate_sentiment_summary(state)
        legal_summary = _generate_legal_summary(state)
        action_summary = _generate_action_summary(state)
        opportunity_summary = _generate_opportunity_summary(state)

        # ë¶„ì„ ìƒíƒœ í™•ì¸
        analysis_status = {
            "spike_analyzer": "success" if state.get("spike_analysis") else "failed",
            "sentiment": "success" if state.get("sentiment_result") else "failed",
            "causality": "success" if state.get("causality_result") else "skipped",
            "legal_rag": "success" if state.get("legal_risk") else "skipped",
            "playbook": "success" if state.get("playbook") else "failed"
        }

        user_message = "ì¼ë¶€ ë¶„ì„ì´ ì œí•œì ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤." if state.get("error_logs") else None

        spike_analysis = state.get("spike_analysis") or {}
        spike_nature = spike_analysis.get("spike_nature", "neutral")

        # Severity score ê³„ì‚°
        playbook = state.get("playbook") or {}
        priority_map = {"urgent": 10, "high": 7, "medium": 5, "low": 3}
        severity_score = priority_map.get(playbook.get("priority", "low"), 5)

        sentiment_result = state.get("sentiment_result") or {}
        sentiment_shift = sentiment_result.get("sentiment_shift", "stable")
        trend_map = {"worsening": "escalating", "improving": "declining", "stable": "stable"}
        trend_direction = trend_map.get(sentiment_shift, "stable")

        keyword = state["spike_event"]["keyword"]
        summary = f"{keyword} - {spike_nature} ì´ìŠˆ ({playbook.get('situation_type', 'monitoring')})"

        # ExecBrief ìƒì„±
        state["executive_brief"] = {
            "summary": summary,
            "severity_score": severity_score,
            "trend_direction": trend_direction,
            "issue_polarity": spike_nature if spike_nature in ["positive", "negative", "mixed"] else "mixed",
            "spike_summary": spike_summary,
            "sentiment_summary": sentiment_summary,
            "legal_summary": legal_summary,
            "action_summary": action_summary,
            "opportunity_summary": opportunity_summary,
            "analysis_status": analysis_status,
            "user_message": user_message,
            "generated_at": _utcnow_z(),
            "analysis_duration_seconds": round(duration, 2)
        }

        _update_node_insight(state, "exec_brief", "generated")
        return state

    except Exception as e:
        _add_error_log(state, "exec_brief", "exception", f"ExecBrief ìƒì„± ì—ëŸ¬: {str(e)}")
        state["executive_brief"] = {
            "summary": "ë¶„ì„ ì‹¤íŒ¨",
            "severity_score": 5,
            "trend_direction": "stable",
            "issue_polarity": "mixed",
            "spike_summary": None,
            "sentiment_summary": None,
            "legal_summary": None,
            "action_summary": None,
            "opportunity_summary": None,
            "analysis_status": {
                "spike_analyzer": "failed",
                "sentiment": "failed",
                "causality": "failed",
                "legal_rag": "failed",
                "playbook": "failed"
            },
            "user_message": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "generated_at": _utcnow_z(),
            "analysis_duration_seconds": 0.0
        }
        _update_node_insight(state, "exec_brief", "failed")
        return state


# ============================================================
# ExecBrief í—¬í¼
# ============================================================

def _generate_spike_summary(state: AnalysisState) -> Optional[str]:
    spike = state.get("spike_analysis")
    if not spike:
        return None
    warning = f", {spike['partial_data_warning']}" if spike.get("partial_data_warning") else ""
    return (
        f"{spike['spike_rate']}ë°° ê¸‰ë“±, "
        f"{spike['spike_type']} íƒ€ìž…, "
        f"{'Breakout í¬í•¨' if spike['viral_indicators']['has_breakout'] else 'ì¼ë°˜ ê¸‰ë“±'}"
        f"{warning}"
    )


def _generate_sentiment_summary(state: AnalysisState) -> Optional[str]:
    sentiment = state.get("sentiment_result")
    if not sentiment:
        return None
    dist = sentiment["sentiment_distribution"]
    dominant = sentiment["dominant_sentiment"]
    return f"{dominant} {dist[dominant]*100:.0f}%, ë¶„ì„ {sentiment['analyzed_count']}ê±´"


def _generate_legal_summary(state: AnalysisState) -> str:
    legal = state.get("legal_risk")
    if not legal:
        return "ë²•ë¥  ê²€í†  ë¯¸ìˆ˜í–‰"
    if legal["clearance_status"] == "clear":
        return "ë²•ì  ë¦¬ìŠ¤í¬ ì—†ìŒ"
    if legal["clearance_status"] == "review_needed":
        return f"ë²•ì  ê²€í†  í•„ìš” ({legal['overall_risk_level']} ë¦¬ìŠ¤í¬)"
    return f"ë²•ì  ë¦¬ìŠ¤í¬ ë†’ìŒ ({legal['overall_risk_level']})"


def _generate_action_summary(state: AnalysisState) -> Optional[str]:
    playbook = state.get("playbook")
    if not playbook or not playbook.get("recommended_actions"):
        return None
    primary_action = playbook["recommended_actions"][0]
    return f"{primary_action['description']} ({primary_action['urgency']} ìš°ì„ ìˆœìœ„)"


def _generate_opportunity_summary(state: AnalysisState) -> Optional[str]:
    spike = state.get("spike_analysis") or {}
    if spike.get("spike_nature") != "positive":
        return None
    playbook = state.get("playbook") or {}
    opportunities = playbook.get("key_opportunities", [])
    if not opportunities:
        return None
    return ", ".join(opportunities[:2])
