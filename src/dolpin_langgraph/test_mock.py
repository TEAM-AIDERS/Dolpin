"""
Mock í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Stub êµ¬í˜„ìœ¼ë¡œ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
    python src/langgraph/test_mock.py
    ë˜ëŠ”
    python -m src.langgraph.test_mock

TODO: ì‹¤ì œ Agent êµ¬í˜„ ì™„ë£Œ í›„
- nodes.pyì˜ Stubì„ ì‹¤ì œ Agent í˜¸ì¶œë¡œ êµì²´
- pytest ê¸°ë°˜ unit testë¡œ ì „í™˜
- ì—£ì§€ ì¼€ì´ìŠ¤ ì¶”ê°€ (mixed sentiment, fanwar ë“±)
"""

import json
import sys
from pathlib import Path
from datetime import datetime, timezone
from unittest.mock import patch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.dolpin_langgraph.graph import compile_workflow
from src.dolpin_langgraph.state import AnalysisState, SpikeEvent, Message


def create_mock_spike_event() -> SpikeEvent:
    """Mock SpikeEvent ìƒì„±"""
    return {
        "keyword": "ë‰´ì§„ìŠ¤",
        "spike_rate": 3.5,
        "baseline": 100,
        "current_volume": 350,
        "detected_at": "2026-01-10T10:00:00Z",
        "time_window": "1h",
        "messages": [
            {
                "id": "550e8400-e29b-41d4-a716-446655440001",
                "source_message_id": "187654321",
                "text": "ë‰´ì§„ìŠ¤ ì»´ë°± ëŒ€ë°•!",
                "timestamp": "2026-01-10T10:00:00Z",
                "source": "twitter",
                "author_id": "user_123",
                "metrics": {"likes": 100, "retweets": 50},
                "is_anonymized": False,
                "detected_language": "ko"
            },
            {
                "id": "550e8400-e29b-41d4-a716-446655440002",
                "source_message_id": "187654322",
                "text": "ë‰´ì§„ìŠ¤ ìµœê³ ì•¼ ã… ã… ",
                "timestamp": "2026-01-10T10:05:00Z",
                "source": "twitter",
                "author_id": "user_456",
                "metrics": {"likes": 200, "retweets": 80},
                "is_anonymized": False,
                "detected_language": "ko"
            }
        ],
        "raw_kafka_message_ids": [
            "550e8400-e29b-41d4-a716-446655440000"
        ]
    }


def create_initial_state(spike_event: SpikeEvent, trace_id: str = "test-trace-001") -> AnalysisState:
    """ì´ˆê¸° State ìƒì„±"""
    return {
        # ì…ë ¥
        "spike_event": spike_event,
        
        # ë©”íƒ€ë°ì´í„°
        "trace_id": trace_id,
        "workflow_start_time": datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),
        
        # Router ê²°ì • (ì´ˆê¸°ê°’ None)
        "route1_decision": None,
        "route2_decision": None,
        "route3_decision": None,
        "positive_viral_detected": None,
        
        # ê° ì—ì´ì „íŠ¸ ê²°ê³¼ (ì´ˆê¸°ê°’ None)
        "spike_analysis": None,
        "sentiment_result": None,
        "causality_result": None,
        "legal_risk": None,
        "amplification_summary": None,
        "playbook": None,
        
        # ë…¸ë“œ ì¸ì‚¬ì´íŠ¸
        "node_insights": {},
        
        # ìµœì¢… ì¶œë ¥
        "executive_brief": None,
        
        # ì—ëŸ¬ ë¡œê·¸
        "error_logs": [],
        
        # Skip ì²˜ë¦¬
        "skipped": False,
        "skip_reason": None
    }


def print_state_summary(state: AnalysisState):
    """State ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š DOLPIN ë¶„ì„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê²°ê³¼")
    print("="*80)
    
    # ë©”íƒ€ë°ì´í„°
    print(f"\nğŸ” Trace ID: {state['trace_id']}")
    print(f"â±ï¸  ì‹œì‘ ì‹œê°„: {state['workflow_start_time']}")
    
    # Router ê²°ì •
    print(f"\nğŸ”€ Router ê²°ì •:")
    print(f"   Router 1ì°¨: {state.get('route1_decision')}")
    print(f"   Router 2ì°¨: {state.get('route2_decision')}")
    print(f"   Router 3ì°¨: {state.get('route3_decision')}")
    print(f"   ê¸ì • ë°”ì´ëŸ´: {state.get('positive_viral_detected')}")
    
    # Skip ì—¬ë¶€
    if state.get("skipped"):
        print(f"\nâ­ï¸  Skip: {state['skip_reason']}")
        return
    
    # ê° ë…¸ë“œ ì¸ì‚¬ì´íŠ¸
    if state.get("node_insights"):
        print(f"\nğŸ’¡ ë…¸ë“œ ì¸ì‚¬ì´íŠ¸:")
        for node, insight in state["node_insights"].items():
            print(f"   â€¢ {node}: {insight}")
    
    # ìµœì¢… ë¸Œë¦¬í•‘
    if state.get("executive_brief"):
        brief = state["executive_brief"]
        print(f"\nğŸ“‹ ì„ì› ë¸Œë¦¬í•‘:")
        print(f"   ìš”ì•½: {brief['summary']}")
        print(f"   ì‹¬ê°ë„: {brief['severity_score']}/10")
        print(f"   íŠ¸ë Œë“œ: {brief['trend_direction']}")
        print(f"   ê·¹ì„±: {brief['issue_polarity']}")
        print(f"   ì†Œìš” ì‹œê°„: {brief['analysis_duration_seconds']}ì´ˆ")
        
        if brief.get("spike_summary"):
            print(f"\n   ğŸ“ˆ Spike: {brief['spike_summary']}")
        if brief.get("sentiment_summary"):
            print(f"   ğŸ˜Š Sentiment: {brief['sentiment_summary']}")
        if brief.get("legal_summary"):
            print(f"   âš–ï¸  Legal: {brief['legal_summary']}")
        if brief.get("action_summary"):
            print(f"   ğŸ¯ Action: {brief['action_summary']}")
        if brief.get("opportunity_summary"):
            print(f"   ğŸŒŸ Opportunity: {brief['opportunity_summary']}")
        
        print(f"\n   ë¶„ì„ ìƒíƒœ:")
        for agent, status in brief["analysis_status"].items():
            emoji = "âœ…" if status == "success" else "â­ï¸" if status == "skipped" else "âŒ"
            print(f"      {emoji} {agent}: {status}")
        
        if brief.get("user_message"):
            print(f"\n   âš ï¸  ì‚¬ìš©ì ë©”ì‹œì§€: {brief['user_message']}")
    
    # ì—ëŸ¬ ë¡œê·¸
    if state.get("error_logs"):
        print(f"\nâŒ ì—ëŸ¬ ë¡œê·¸ ({len(state['error_logs'])}ê±´):")
        for error in state["error_logs"]:
            print(f"   â€¢ [{error['stage']}] {error['error_type']}: {error['message']}")
    
    print("\n" + "="*80 + "\n")


def save_result_to_file(state: AnalysisState, filename: str = "mock_result.json"):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    # tests/outputs ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(__file__).parent.parent.parent / "tests" / "outputs"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # íŒŒì¼ ê²½ë¡œ
    filepath = output_dir / filename
    
    # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    def convert_datetime(obj):
        if isinstance(obj, datetime):
            return obj.isoformat() + "Z"
        return obj
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2, default=convert_datetime)
    
    print(f"ğŸ’¾ ê²°ê³¼ê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def run_test_case_1_positive_viral():
    """
    í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ê¸ì • ë°”ì´ëŸ´ (Amplification ê²½ë¡œ)
    
    ì¡°ê±´:
    - spike_rate: 3.5 (high)
    - spike_nature: "positive"
    - support: 0.6 (>= 0.5)
    - actionability_score: 0.7 (>= 0.6)
    
    ì˜ˆìƒ ê²½ë¡œ:
    SpikeAnalyzer â†’ Router1 (analyze) â†’ Sentiment â†’ Router2 (full_analysis) 
    â†’ Causality â†’ Router3 (amplification) â†’ Amplification â†’ Playbook â†’ ExecBrief
    
    TODO: ì‹¤ì œ Agent êµ¬í˜„ ì‹œ Stub ë°ì´í„°ë¥¼ ì‹¤ì œ ë¶„ì„ ê²°ê³¼ë¡œ êµì²´
    """
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ê¸ì • ë°”ì´ëŸ´ (Amplification ê²½ë¡œ)")
    print("-" * 80)
    
    spike_event = create_mock_spike_event()
    initial_state = create_initial_state(spike_event, trace_id="test-trace-001")
    
    # ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ë° ì‹¤í–‰
    app = compile_workflow()
    final_state = app.invoke(initial_state)
    
    # ê²°ê³¼ ì¶œë ¥
    print_state_summary(final_state)
    
    # ê²€ì¦
    assert final_state["route1_decision"] == "analyze", "Router 1ì°¨ê°€ analyzeì—¬ì•¼ í•¨"
    assert final_state["route2_decision"] == "full_analysis", "Router 2ì°¨ê°€ full_analysisì—¬ì•¼ í•¨"
    assert final_state["route3_decision"] == "amplification", "Router 3ì°¨ê°€ amplificationì´ì–´ì•¼ í•¨"
    assert final_state["positive_viral_detected"] == True, "ê¸ì • ë°”ì´ëŸ´ ê°ì§€ë˜ì–´ì•¼ í•¨"
    
    print("âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1 í†µê³¼!")
    
    return final_state


def run_test_case_2_skip():
    """
    í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: Skip ê²½ë¡œ
    
    ì¡°ê±´:
    - is_significant: False (ê¸‰ë“± ë¯¸ë‹¬)
    
    ì˜ˆìƒ ê²½ë¡œ:
    SpikeAnalyzer â†’ Router1 (skip) â†’ ì¢…ë£Œ
    
    TODO: ì‹¤ì œ SpikeAnalyzer êµ¬í˜„ ì‹œ spike_rate ê¸°ë°˜ìœ¼ë¡œ is_significant íŒë‹¨
    """
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: Skip ê²½ë¡œ")
    print("-" * 80)
    
    spike_event = create_mock_spike_event()
    spike_event["spike_rate"] = 1.1  # ë‚®ì€ ê¸‰ë“±ë¥ 
    
    initial_state = create_initial_state(spike_event, trace_id="test-trace-002")
    
    # TODO: í˜„ì¬ Stubì€ í•­ìƒ is_significant=True
    # ì‹¤ì œ êµ¬í˜„ ì‹œ spike_rate ê¸°ë°˜ íŒë‹¨ í•„ìš”
    print("âš ï¸  í˜„ì¬ Stubì€ í•­ìƒ is_significant=Trueë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    print("   ì‹¤ì œ SpikeAnalyzer êµ¬í˜„ ì‹œ spike_rate ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨í•  ì˜ˆì •ì…ë‹ˆë‹¤.")
    
    # ìˆ˜ë™ìœ¼ë¡œ skip ìƒíƒœ ì„¤ì • (í…ŒìŠ¤íŠ¸ìš©)
    initial_state["spike_analysis"] = {
        "is_significant": False,
        "spike_rate": 1.1,
        "spike_type": "organic",
        "spike_nature": "neutral",
        "peak_timestamp": "2026-01-10T10:30:00Z",
        "duration_minutes": 30,
        "confidence": 0.9,
        "actionability_score": 0.1,
        "data_completeness": "confirmed",
        "partial_data_warning": None,
        "viral_indicators": {
            "is_trending": False,
            "has_breakout": False,
            "max_rise_rate": "+10%",
            "breakout_queries": [],
            "cross_platform": ["twitter"],
            "international_reach": 0.0
        }
    }
    
    # Router1ë§Œ ì‹¤í–‰
    from src.dolpin_langgraph.nodes import router1_node
    final_state = router1_node(initial_state)
    
    print_state_summary(final_state)
    
    assert final_state["route1_decision"] == "skip", "Router 1ì°¨ê°€ skipì´ì–´ì•¼ í•¨"
    assert final_state["skipped"] == True, "skipped í”Œë˜ê·¸ê°€ Trueì—¬ì•¼ í•¨"
    
    print("âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2 í†µê³¼!")
    
    return final_state


def run_test_case_3_sentiment_only():
    """
    í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: Sentiment Only ê²½ë¡œ (ë‚®ì€ actionability)
    
    ì¡°ê±´:
    - actionability_score: 0.2 (< 0.3)
    - ë‹¨ìˆœ íŒ¬ ë°˜ì‘, ìœ„ê¸°/ê¸°íšŒ ì‹ í˜¸ ì—†ìŒ
    
    ì˜ˆìƒ ê²½ë¡œ:
    SpikeAnalyzer â†’ Router1 (analyze) â†’ Sentiment â†’ Router2 (sentiment_only) 
    â†’ Playbook â†’ ExecBrief
    
    TODO: ì‹¤ì œ êµ¬í˜„ ì‹œ nodes.pyì˜ Stubì„ ì‹¤ì œ Agentë¡œ êµì²´
    """
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: Sentiment Only ê²½ë¡œ (ë‚®ì€ actionability)")
    print("-" * 80)
    
    spike_event = create_mock_spike_event()
    spike_event["keyword"] = "ì—ìŠ¤íŒŒ ì˜ˆì˜ë‹¤"  # ë‹¨ìˆœ íŒ¬ ë°˜ì‘
    initial_state = create_initial_state(spike_event, trace_id="test-trace-003")
    
    # TODO: nodes.pyì˜ spike_analyzer_node Stub ìˆ˜ì • í•„ìš”
    # actionability_scoreë¥¼ 0.2ë¡œ ì„¤ì •
    
    def mock_spike_node(state):
        state["spike_analysis"] = {
            "is_significant": True,
            "spike_rate": 2.0,
            "spike_type": "organic",
            "spike_nature": "positive",
            "peak_timestamp": "2026-01-10T10:30:00Z",
            "duration_minutes": 30,
            "confidence": 0.9,
            "actionability_score": 0.2,  # í•µì‹¬: ë‚®ì€ actionability
            "data_completeness": "confirmed",
            "partial_data_warning": None,
            "viral_indicators": {
                "is_trending": False,
                "has_breakout": False,
                "max_rise_rate": "+20%",
                "breakout_queries": [],
                "cross_platform": ["twitter"],
                "international_reach": 0.1
            }
        }
        return state

    def mock_sentiment_node(state):
        state["sentiment_result"] = {
            "dominant_sentiment": "support",
            "sentiment_distribution": {
                "support": 0.8,
                "neutral": 0.2
            },
            "confidence": 0.85,
            "sentiment_shift": "stable",
            "analyzed_count": 10,
            "representative_messages": {
                "support": ["ì—ìŠ¤íŒŒ ì§„ì§œ ì˜ˆì˜ë‹¤ ã… ã… "]
            }
        }
        return state

    with patch("src.dolpin_langgraph.nodes.spike_analyzer_node", mock_spike_node), \
         patch("src.dolpin_langgraph.nodes.sentiment_node", mock_sentiment_node):

        app = compile_workflow()
        final_state = app.invoke(initial_state)

    print_state_summary(final_state)
    
    assert final_state["route1_decision"] == "analyze", \
        "Router 1ì°¨ëŠ” analyzeì—¬ì•¼ í•¨"

    assert final_state["route2_decision"] == "sentiment_only", \
        "Router 2ì°¨ëŠ” sentiment_onlyì—¬ì•¼ í•¨"

    assert final_state["route3_decision"] is None, \
        "Router 3ì°¨ëŠ” ìˆ˜í–‰ë˜ì§€ ì•Šì•„ì•¼ í•¨"

    print("âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3 í†µê³¼!")

    return final_state



def run_test_case_4_legal_crisis():
    """
    í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: Legal ê²½ë¡œ (ìœ„ê¸° ì‹ í˜¸ - ë³´ì´ì½§)
    
    ì¡°ê±´:
    - actionability_score: 0.5 (ì¤‘ê°„)
    - boycott: 0.3 (>= 0.2) â†’ ìœ„ê¸° ì‹ í˜¸
    - spike_nature: "negative"
    
    ì˜ˆìƒ ê²½ë¡œ:
    SpikeAnalyzer â†’ Router1 (analyze) â†’ Sentiment â†’ Router2 (full_analysis)
    â†’ Causality â†’ Router3 (legal) â†’ Legal RAG â†’ Playbook â†’ ExecBrief
    
    TODO: ì‹¤ì œ êµ¬í˜„ ì‹œ nodes.pyì˜ Stubì„ ì‹¤ì œ Agentë¡œ êµì²´
    """
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: Legal ê²½ë¡œ (ìœ„ê¸° ì‹ í˜¸ - ë³´ì´ì½§)")
    print("-" * 80)
    
    spike_event = create_mock_spike_event()
    spike_event["keyword"] = "XX ë¶ˆë§¤"
    initial_state = create_initial_state(spike_event, trace_id="test-trace-004")
    
    # TODO: nodes.py Stub ìˆ˜ì • í•„ìš”
    # spike_analyzer_node: spike_nature="negative", actionability_score=0.5
    # sentiment_node: boycott=0.3

    def mock_spike_node(state):
        state["spike_analysis"] = {
            "is_significant": True,
            "spike_rate": 2.5,
            "spike_type": "organic",
            "spike_nature": "negative",   # ìœ„ê¸°
            "peak_timestamp": "2026-01-10T10:30:00Z",
            "duration_minutes": 60,
            "confidence": 0.9,
            "actionability_score": 0.5,   # ì¤‘ê°„ êµ¬ê°„
            "data_completeness": "confirmed",
            "partial_data_warning": None,
            "viral_indicators": {
                "is_trending": True,
                "has_breakout": False,
                "max_rise_rate": "+150%",
                "breakout_queries": [],
                "cross_platform": ["twitter"],
                "international_reach": 0.2
            }
        }
        return state

    def mock_sentiment_node(state):
        state["sentiment_result"] = {
            "dominant_sentiment": "boycott",
            "sentiment_distribution": {
                "boycott": 0.3,
                "support": 0.2,
                "neutral": 0.3,
                "fanwar": 0.0,
                "disappointment": 0.2
            },
            "confidence": 0.85,
            "sentiment_shift": "worsening",
            "analyzed_count": 50,
            "has_mixed_sentiment": True,
            "representative_messages": {
                "boycott": ["ì´ë²ˆ í™œë™ì€ ë¶ˆë§¤í•œë‹¤"]
            }
        }
        return state

    with patch("src.dolpin_langgraph.nodes.spike_analyzer_node", mock_spike_node), \
         patch("src.dolpin_langgraph.nodes.sentiment_node", mock_sentiment_node):

        app = compile_workflow()
        final_state = app.invoke(initial_state)

    print_state_summary(final_state)

    assert final_state["route2_decision"] == "full_analysis", \
        "Router 2ì°¨ê°€ full_analysisì—¬ì•¼ í•¨"

    assert final_state["route3_decision"] == "legal", \
        "Router 3ì°¨ê°€ legalì´ì–´ì•¼ í•¨"

    print("âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4 í†µê³¼!")

    return final_state


def run_test_case_5_legal_keyword():
    """
    í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5: Legal ê²½ë¡œ (ë²•ì  í‚¤ì›Œë“œ ê°ì§€)
    
    ì¡°ê±´:
    - dominant_sentiment: "meme_negative"
    - ëª…ì˜ˆí›¼ì† í‚¤ì›Œë“œ í¬í•¨
    - legal_risk: high_risk
    
    ì˜ˆìƒ ê²½ë¡œ:
    SpikeAnalyzer â†’ Router1 (analyze) â†’ Sentiment â†’ Router2 (full_analysis)
    â†’ Causality â†’ Router3 (legal) â†’ Legal RAG (high_risk) â†’ Playbook â†’ ExecBrief
    
    TODO: ì‹¤ì œ êµ¬í˜„ ì‹œ nodes.pyì˜ Stubì„ ì‹¤ì œ Agentë¡œ êµì²´
    """
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5: Legal ê²½ë¡œ (ë²•ì  í‚¤ì›Œë“œ)")
    print("-" * 80)
    
    spike_event = create_mock_spike_event()
    spike_event["keyword"] = "XX ëª…ì˜ˆí›¼ì†"
    
    # ì‹¤ì œ ë©”ì‹œì§€ì— í‚¤ì›Œë“œ í¬í•¨ (LegalRAG quick check í†µê³¼ìš©)
    spike_event["messages"] = [
        {
            "id": "1",
            "source_message_id": "1",
            "text": "ì´ê±´ ì§„ì§œ ëª…ì˜ˆí›¼ì†ì´ë‹¤",
            "timestamp": "2026-01-10T10:00:00Z",
            "source": "twitter",
            "author_id": "user1",
            "metrics": {},
            "is_anonymized": False,
            "detected_language": "ko"
        }
    ]
    initial_state = create_initial_state(spike_event, trace_id="test-trace-005")
    
    # TODO: nodes.py Stub ìˆ˜ì • í•„ìš”
    # sentiment_node: dominant_sentiment="meme_negative"
    # legal_rag_node: clearance_status="high_risk"

    def mock_spike_node(state):
        state["spike_analysis"] = {
            "is_significant": True,
            "spike_rate": 2.8,
            "spike_type": "organic",
            "spike_nature": "negative",
            "peak_timestamp": "2026-01-10T10:30:00Z",
            "duration_minutes": 45,
            "confidence": 0.9,
            "actionability_score": 0.6,  # full_analysis ìœ ë„
            "data_completeness": "confirmed",
            "partial_data_warning": None,
            "viral_indicators": {
                "is_trending": True,
                "has_breakout": False,
                "max_rise_rate": "+180%",
                "breakout_queries": [],
                "cross_platform": ["twitter"],
                "international_reach": 0.1
            }
        }
        return state

    def mock_sentiment_node(state):
        state["sentiment_result"] = {
            "dominant_sentiment": "meme_negative",
            "sentiment_distribution": {
                "meme_negative": 0.4,
                "support": 0.2,
                "neutral": 0.3,
                "boycott": 0.0,
                "fanwar": 0.0
            },
            "confidence": 0.8,
            "sentiment_shift": "worsening",
            "analyzed_count": 40,
            "has_mixed_sentiment": True,
            "representative_messages": {
                "meme_negative": ["ì´ê±´ ì§„ì§œ ëª…ì˜ˆí›¼ì†ì´ë‹¤"]
            }
        }
        return state

    with patch("src.dolpin_langgraph.nodes.spike_analyzer_node", mock_spike_node), \
         patch("src.dolpin_langgraph.nodes.sentiment_node", mock_sentiment_node):

        app = compile_workflow()
        final_state = app.invoke(initial_state)
    
    print_state_summary(final_state)

    assert final_state["route2_decision"] == "full_analysis", \
        "Router 2ì°¨ê°€ full_analysisì—¬ì•¼ í•¨"

    assert final_state["route3_decision"] == "legal", \
        "Router 3ì°¨ê°€ legalì´ì–´ì•¼ í•¨"

    assert final_state["legal_risk"]["clearance_status"] == "high_risk", \
        "Legal RAG ê²°ê³¼ê°€ high_riskì—¬ì•¼ í•¨"

    print("âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5 í†µê³¼!")

    return final_state

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\n" + "="*80)
    print("ğŸš€ DOLPIN Mock ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*80)
    print("\nğŸ’¡ í˜„ì¬ ì‹¤í–‰ ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸: 1, 2, 3, 4, 5")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ê¸ì • ë°”ì´ëŸ´
    result1 = run_test_case_1_positive_viral()
    if result1:
        save_result_to_file(result1, "mock_result_case1_positive_viral.json")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: Skip
    result2 = run_test_case_2_skip()
    if result2:
        save_result_to_file(result2, "mock_result_case2_skip.json")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: Sentiment Only
    run_test_case_3_sentiment_only()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: Legal - Crisis 
    run_test_case_4_legal_crisis()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5: Legal - Keyword
    run_test_case_5_legal_keyword()
    
    print("\n" + "="*80)
    print("ğŸ‰ ëª¨ë“  Mock í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì™„ë£Œ! (5/5)")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()
