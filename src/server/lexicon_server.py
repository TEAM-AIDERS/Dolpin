"""
Custom Lexicon MCP Server - ê³µí†µ ì°¸ì¡° ì§€ì‹ ë² ì´ìŠ¤
íŒ¬ë¤ íŠ¹í™” í‘œí˜„ì„ ì¡°íšŒí•˜ëŠ” Read-only Lookup MCP

ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ íŒ¬ë¤ íŠ¹í™” í‘œí˜„, í–‰ë™ íŠ¸ë¦¬ê±°, ë§¥ë½ ì‹ í˜¸ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ì œê³µí•œë‹¤.
íŒë‹¨ì´ë‚˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì§€ ì•Šìœ¼ë©°, í•˜ìœ„ Agentë“¤ì´ í™œìš©í•  ì •ì  ì§€ì‹ ì¡°íšŒ ì¸í„°í˜ì´ìŠ¤ë‹¤.

ë²„ì „: 260203 - entry ìƒì„± ì‹¤íŒ¨ ì‹œ fallback ìƒì„± ë¡œì§ í¬í•¨
"""

import logging
import json 
import pandas as pd
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Literal
from dataclasses import dataclass, asdict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================
# 1. ë°ì´í„° ëª¨ë¸
# ============================================================

@dataclass
class LexiconEntry:
    """CSVì—ì„œ ë¡œë“œí•œ íŒ¬ë¤ íŠ¹í™” í‘œí˜„ ì—”íŠ¸ë¦¬"""
    term: str
    normalized_form: str
    type: Literal["boycott_action", "context_marker", "fandom_slang"]
    sentiment_label: str
    trigger_type: str  # action, emotion, context
    action_strength: str  # collective, declaration, none
    fandom_scope: str  # fandom, global
    target_entity: str  # agency, artist, self, fandom, unknown
    polarity: Literal["positive", "negative", "neutral"]
    intensity: Literal["high", "mid", "low"]
    risk_flag: Literal["alert", "watch", "none"]
    example_text: str
    usage_mode: str  # literal, ambiguous
    notes: str
    created_at: str
    updated_at: str


@dataclass
class LexiconMatch:
    """í…ìŠ¤íŠ¸ ë§¤ì¹­ ê²°ê³¼"""
    term: str
    entry: LexiconEntry
    context_window: str  # ë§¤ì¹­ ë¬¸ë§¥


@dataclass
class AnalysisContext:
    """Agentë“¤ì˜ ê³µí†µ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸"""
    text: str
    matches: List[LexiconMatch]
    aggregated_signals: Dict[str, Any]  # sentiment, triggers, risks ì¢…í•©


# ============================================================
# 2. LexiconServer í´ë˜ìŠ¤
# ============================================================

class LexiconServer:
    """íŒ¬ë¤ íŠ¹í™” í‘œí˜„ ë°ì´í„°ë² ì´ìŠ¤ - MCP ë„êµ¬ì˜ ê¸°ë°˜"""
    
    def __init__(self, csv_path: str = "custom_lexicon.csv"):
        """CSV íŒŒì¼ë¡œë¶€í„° lexicon ë¡œë“œ"""
        self.csv_path = Path(csv_path)
        
        # 1. ì¸ì½”ë”© ì²˜ë¦¬ + ëª…ì‹œ ë¡œê·¸
        try:
            self.df = pd.read_csv(self.csv_path, encoding='utf-8')
            self.encoding_used = 'utf-8'
        except UnicodeDecodeError:
            logger.warning(f"UTF-8 decode failed for {self.csv_path}, trying cp949...")
            self.df = pd.read_csv(self.csv_path, encoding='cp949')
            self.encoding_used = 'cp949'
        
        logger.info(f"Loaded {len(self.df)} lexicon entries from {csv_path}")
        logger.info(f"Encoding used: {self.encoding_used}")
        
        # 2. CSV Validation (ë¬´íš¨í•œ í–‰ í•„í„°ë§)
        self._validate_csv()
        
        # 3. íš¨ìœ¨ì„±ì„ ìœ„í•œ ì¸ë±ì‹±
        self.by_term = self._index_by_term()  # ì¤‘ë³µ ì§€ì›
        self.by_type = self._index_by_column('type')
        self.by_trigger = self._index_by_column('trigger_type')
        self.by_risk = self._index_by_column('risk_flag')
        
        # 4. ì„±ëŠ¥ ê°œì„ : ì •ê·œì‹ ì»´íŒŒì¼ (ê¸¸ì´ìˆœ)
        self._compile_term_regex()
    
    def _validate_csv(self):
        """CSV ë°ì´í„° ê²€ì¦ ë° í•„í„°ë§ (Flexible ëª¨ë“œ)"""
        # í•„ìˆ˜ ì»¬ëŸ¼: termê³¼ sentiment_labelë§Œ í•„ìˆ˜
        required_columns = ['term', 'sentiment_label']
        
        missing = set(required_columns) - set(self.df.columns)
        if missing:
            raise ValueError(f"Missing required columns: {missing}")
        
        # 1. ë¹ˆ term ì œê±° (ìœ ì¼í•œ ì—„ê²© ì²´í¬)
        before_count = len(self.df)
        self.df = self.df[self.df['term'].notna() & (self.df['term'] != '')]
        removed_empty = before_count - len(self.df)
        if removed_empty > 0:
            logger.info(f"Removed {removed_empty} rows with empty term")
        
        # 2. ì„ íƒì  í•„ë“œ ì •ê·œí™” (ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°)
        defaults = {
            'type': 'unknown',
            'polarity': 'neutral',
            'intensity': 'low',
            'risk_flag': 'none',
            'trigger_type': 'context',
            'action_strength': 'none',
            'fandom_scope': 'global',
            'target_entity': 'unknown',
            'normalized_form': 'term',
            'example_text': '',
            'usage_mode': 'literal',
            'notes': '',
            'created_at': '',
            'updated_at': ''
        }
        
        for col, default in defaults.items():
            if col not in self.df.columns:
                self.df[col] = default
            else:
                # NaNê³¼ ë¹ˆ ë¬¸ìì—´ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
                if col == 'normalized_form':
                    # normalized_formì€ term ê°’ìœ¼ë¡œ ì‚¬ìš©
                    self.df[col] = self.df[col].fillna(self.df['term'])
                    # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
                    mask = self.df[col].apply(lambda x: isinstance(x, str) and x.strip() == '')
                    self.df.loc[mask, col] = self.df.loc[mask, 'term']
                else:
                    self.df[col] = self.df[col].fillna(default)
                    self.df[col] = self.df[col].apply(
                        lambda x: default if (isinstance(x, str) and x.strip() == '') else x
                    )
        
        # 3. ë™ì  ìœ íš¨ê°’ í•™ìŠµ (í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹ )
        self.valid_types = set(self.df['type'].unique())
        self.valid_polarities = set(self.df['polarity'].unique())
        self.valid_intensities = set(self.df['intensity'].unique())
        self.valid_risks = set(self.df['risk_flag'].unique())
        
        # 4. í†µê³„ ì •ë³´ ë¡œê¹…
        logger.info(f"CSV Validation Complete:")
        logger.info(f"  - Total rows: {len(self.df)}")
        logger.info(f"  - Types discovered: {self.valid_types}")
        logger.info(f"  - Polarities: {self.valid_polarities}")
        logger.info(f"  - Intensities: {self.valid_intensities}")
        logger.info(f"  - Risk flags: {self.valid_risks}")

    
    def _index_by_term(self) -> Dict[str, List[Dict]]:
        """term ì¸ë±ì‹± (ì¤‘ë³µ term ì§€ì›)"""
        index = {}
        for _, row in self.df.iterrows():
            term = str(row['term']).strip()
            if term not in index:
                index[term] = []
            index[term].append(row.to_dict())
        
        # ì¤‘ë³µ term ë¡œê·¸
        duplicates = {t: len(rows) for t, rows in index.items() if len(rows) > 1}
        if duplicates:
            logger.warning(f"Duplicate terms found: {duplicates}")
        
        return index
    
    def _compile_term_regex(self):
        """ìµœì í™”: ê¸¸ì´ ê¸´ termë¶€í„° ì •ê·œì‹ ì»´íŒŒì¼ (ìº¡ì²˜ ê·¸ë£¹ ì œê±°)"""
        # ê¸¸ì´ ì—­ìˆœ ì •ë ¬ (ê¸´ term ë¨¼ì € ë§¤ì¹­)
        terms_sorted = sorted(
            self.df['term'].unique(),
            key=lambda t: len(str(t)),
            reverse=True
        )
        
        try:
            # ìº¡ì²˜ ê·¸ë£¹ () ì œê±° - ê°„ë‹¨í•œ íŒ¨í„´
            escaped_terms = [re.escape(str(term)) for term in terms_sorted]
            pattern = "|".join(escaped_terms)  # â† ê·¸ë£¹ ì œê±°
            self.term_pattern = re.compile(pattern, re.IGNORECASE)
            self.terms_sorted = terms_sorted
            
            # lower ë³€í™˜ lookup dict (O(1) ë§¤ì¹˜)
            self.term_lower_to_term = {str(t).lower(): t for t in terms_sorted}
            
            logger.info(f"Compiled regex pattern for {len(terms_sorted)} terms")
        except Exception as e:
            logger.error(f"Failed to compile term regex: {e}")
            self.term_pattern = None
            self.terms_sorted = terms_sorted
            self.term_lower_to_term = {str(t).lower(): t for t in terms_sorted}
    
    def _index_by_column(self, column: str) -> Dict[str, List[Dict]]:
        """ì»¬ëŸ¼ ê¸°ë°˜ ì¸ë±ì‹±"""
        index = {}
        for _, row in self.df.iterrows():
            key = row[column]
            if key not in index:
                index[key] = []
            index[key].append(row.to_dict())
        return index
    
    # ========== ê¸°ë³¸ ì¡°íšŒ ë„êµ¬ ==========
    
    def lookup_term(self, term: str) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ìš©ì–´ ì¡°íšŒ (ì²« ë²ˆì§¸ í•­ëª©)"""
        term_clean = str(term).strip()
        if term_clean in self.by_term and len(self.by_term[term_clean]) > 0:
            return self.by_term[term_clean][0]
        return None
    
    def lookup_terms(self, terms: List[str]) -> List[Dict[str, Any]]:
        """ë‹¤ì¤‘ ìš©ì–´ ì¡°íšŒ"""
        results = []
        for term in terms:
            entry = self.lookup_term(term)
            if entry:
                results.append(entry)
        return results
    
    def search_by_type(self, type_: str) -> List[Dict[str, Any]]:
        """íƒ€ì…ë³„ ì¡°íšŒ (boycott_action, context_marker, fandom_slang)"""
        return self.by_type.get(type_, [])
    
    def search_by_trigger(self, trigger_type: str) -> List[Dict[str, Any]]:
        """íŠ¸ë¦¬ê±° íƒ€ì…ë³„ ì¡°íšŒ (action, emotion, context)"""
        return self.by_trigger.get(trigger_type, [])
    
    def search_by_risk(self, risk_flag: str) -> List[Dict[str, Any]]:
        """ìœ„í—˜ë„ ìˆ˜ì¤€ë³„ ì¡°íšŒ (alert, watch, none)"""
        return self.by_risk.get(risk_flag, [])
    
    # ========== ë¶„ì„ìš© ë„êµ¬ ==========
    
    def extract_matches(self, text: str) -> List[LexiconMatch]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ë§¤ì¹­ë˜ëŠ” ìš©ì–´ ì¶”ì¶œ

        ê°œì„ ì‚¬í•­:
        - LexiconEntry ìƒì„± ì‹¤íŒ¨ ì‹œ fallback entry ìƒì„±
        - ë§¤ì¹˜ê°€ ì†ì‹¤ë˜ì§€ ì•Šë„ë¡ ë³´ì¥
        """
        matches = []
        
        # text ì›ë³¸ ì‚¬ìš© (ì¸ë±ìŠ¤ê°€ ì›ë¬¸ ê¸°ì¤€)
        if self.term_pattern:
            for match in self.term_pattern.finditer(text): 
                matched_term_lower = match.group(0).lower()
                start_idx = match.start()
                end_idx = match.end()
                
                # context window ì¶”ì¶œ (ì–‘ìª½ 30ì)
                ctx_start = max(0, start_idx - 30)
                ctx_end = min(len(text), end_idx + 30)
                context = text[ctx_start:ctx_end]
                
                # O(1) ë§¤ì¹˜: dict ì¡°íšŒë¡œ ì›ë³¸ term ì°¾ê¸°
                term = self.term_lower_to_term.get(matched_term_lower)
                if term:
                    # ì¤‘ë³µ term ëª¨ë‘ ì²˜ë¦¬
                    for entry_dict in self.by_term.get(term, []):
                        try:
                            entry = LexiconEntry(**entry_dict)
                            matches.append(LexiconMatch(
                                term=term,
                                entry=entry,
                                context_window=context
                            ))
                        except TypeError as e:
                            # ===== Fallback: ê¸°ë³¸ê°’ìœ¼ë¡œ LexiconEntry ìƒì„± =====
                            logger.warning(
                                f"Entry ìƒì„± ì‹¤íŒ¨ (term: {term}): {e}. "
                                f"ê¸°ë³¸ê°’ìœ¼ë¡œ Entryë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
                            )
                        
                            fallback_entry = self._create_fallback_entry(term, entry_dict, error=str(e))
                            matches.append(LexiconMatch(
                                term=term,
                                entry=fallback_entry,
                                context_window=context
                            ))
        else:
            # Fallback: ì •ê·œì‹ ì‹¤íŒ¨ ì‹œ ê¸¸ì´ìˆœ ìˆœíšŒ
            logger.debug("Using fallback term matching (regex unavailable)")
            text_lower = text.lower()
            for term in self.terms_sorted:
                term_lower = str(term).lower()
                if term_lower in text_lower:
                    idx = text_lower.find(term_lower)
                    start = max(0, idx - 30)
                    end = min(len(text), idx + len(term_lower) + 30)
                    context = text[start:end]
                    
                    for entry_dict in self.by_term.get(term, []):
                        try:
                            entry = LexiconEntry(**entry_dict)
                            matches.append(LexiconMatch(
                                term=term,
                                entry=entry,
                                context_window=context
                            ))
                        except TypeError as e:
                            # ===== Fallback =====
                            logger.warning(
                                f"LexiconEntry ìƒì„± ì‹¤íŒ¨ (term: {term}): {e}. "
                                f"fallback entry ìƒì„±í•©ë‹ˆë‹¤."
                            )
                            fallback_entry = self._create_fallback_entry(term, entry_dict, error=str(e))
                            matches.append(LexiconMatch(
                                term=term,
                                entry=fallback_entry,
                                context_window=context
                            ))
        
        logger.debug(f"Found {len(matches)} matches in text")
        return matches
    
    def _create_fallback_entry(
        self, 
        term: str, 
        entry_dict: Dict[str, Any], 
        error: str
    ) -> LexiconEntry:
        """
        LexiconEntry ìƒì„± ì‹¤íŒ¨ ì‹œ fallback entry ìƒì„±
    
        Args:
            term: ë§¤ì¹­ëœ ìš©ì–´
            entry_dict: ì›ë³¸ dict (ì¼ë¶€ í•„ë“œë§Œ ìˆì„ ìˆ˜ ìˆìŒ)
            error: ì—ëŸ¬ ë©”ì‹œì§€
    
        Returns:
            exiconEntry: ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›Œì§„ fallback entry
        """
        # ì•ˆì „í•œ ê°’ ì¶”ì¶œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)

        def safe_get(key: str, default: Any) -> Any:
            value = entry_dict.get(key, default)
            # Noneì´ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            if value is None or (isinstance(value, str) and value.strip() == ''):
                return default
            return value
    
        # Fallback LexiconEntry ìƒì„±
        fallback = LexiconEntry(
            term=term,
            normalized_form=safe_get('normalized_form', term),  # termìœ¼ë¡œ ëŒ€ì²´
            type=safe_get('type', 'unknown'),  # ê¸°ë³¸ê°’
            sentiment_label=safe_get('sentiment_label', 'neutral'),
            trigger_type=safe_get('trigger_type', 'context'),
            action_strength=safe_get('action_strength', 'none'),
            fandom_scope=safe_get('fandom_scope', 'global'),
            target_entity=safe_get('target_entity', 'unknown'),
            polarity=safe_get('polarity', 'neutral'),
            intensity=safe_get('intensity', 'low'),
            risk_flag=safe_get('risk_flag', 'none'),
            example_text=safe_get('example_text', ''),
            usage_mode=safe_get('usage_mode', 'literal'),
            notes=f"[FALLBACK] {error}",  # ğŸ‘ˆ ì—ëŸ¬ ì •ë³´ ê¸°ë¡
            created_at=safe_get('created_at', ''),
            updated_at=safe_get('updated_at', '')
        )
    
        logger.debug(f"Created fallback entry for term: {term}")
        return fallback

    def analyze_text(self, text: str) -> AnalysisContext:
        """í…ìŠ¤íŠ¸ ì „ì²´ ë¶„ì„ - Agentë“¤ì˜ ì…ë ¥ ì»¨í…ìŠ¤íŠ¸"""
        matches = self.extract_matches(text)
        
        # ì‹ í˜¸ ì¢…í•©
        aggregated = {
            "total_matches": len(matches),
            "matched_terms": [m.term for m in matches],
            "sentiment_mix": self._aggregate_sentiment([m.entry for m in matches]),
            "action_triggers": self._extract_triggers([m.entry for m in matches]),
            "risk_flags": [m.entry.risk_flag for m in matches if m.entry.risk_flag != "none"],
            "target_entities": list(set(m.entry.target_entity for m in matches)),
        }
        
        return AnalysisContext(
            text=text,
            matches=matches,
            aggregated_signals=aggregated
        )
    
    def _aggregate_sentiment(self, entries: List[LexiconEntry]) -> Dict[str, Any]:
        """ê°ì • ì‹ í˜¸ ì¢…í•©"""
        if not entries:
            return {"positive": 0, "negative": 0, "neutral": 0}
        
        sentiment_count = {"positive": 0, "negative": 0, "neutral": 0}
        for e in entries:
            if e.polarity in sentiment_count:
                sentiment_count[e.polarity] += 1
        
        return sentiment_count
    
    def _extract_triggers(self, entries: List[LexiconEntry]) -> List[Dict[str, Any]]:
        """íŠ¸ë¦¬ê±° ì‹ í˜¸ ì¶”ì¶œ (BUG FIX: return ì¶”ê°€!)"""
        triggers = []
        for e in entries:
            if e.trigger_type and e.trigger_type != "none":
                triggers.append({
                    "term": e.term,
                    "trigger_type": e.trigger_type,
                    "action_strength": e.action_strength,
                    "type": e.type,
                })
        return triggers  # âœ… ì´ ì¤„ì´ ì—†ì—ˆìŒ!
    
    # ========== MCP Tool ì‹¤í–‰ ==========
    
    def execute_tool(self, tool_name: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """MCP ë„êµ¬ ì‹¤í–‰ (lookup_term ì§€ì› ì¶”ê°€!)"""
        
        # [1] lookup_term ì§€ì›
        if tool_name == "lookup_term":
            term = params.get("term")
            result = self.lookup_term(term)
            return {
                "tool": "lookup_term",
                "term": term,
                "entry": result,
                "found": result is not None
            }
        
        # [2] lookup_terms ì§€ì›
        elif tool_name == "lookup_terms":
            terms = params.get("terms", [])
            results = self.lookup_terms(terms)
            return {
                "tool": "lookup_terms",
                "terms": terms,
                "entries": results,
                "count": len(results)
            }
        
        # [3] analyze_text (í•µì‹¬)
        elif tool_name == "analyze_text":
            text = params.get("text")
            context = self.analyze_text(text)
            
            matches_data = [
                {
                    "term": m.term,
                    "entry": asdict(m.entry),
                    "context_window": m.context_window
                }
                for m in context.matches
            ]
            
            return {
                "tool": "analyze_text",
                "text": text,
                "matches": matches_data,
                "aggregated_signals": context.aggregated_signals
            }
        
        # [4] get_sentiment_context
        elif tool_name == "get_sentiment_context":
            text = params.get("text")
            context = self.analyze_text(text)
            
            sentiment_signals = [
                {
                    "term": m.term,
                    "polarity": m.entry.polarity,
                    "intensity": m.entry.intensity,
                    "sentiment_label": m.entry.sentiment_label
                }
                for m in context.matches
            ]
            
            return {
                "tool": "get_sentiment_context",
                "text": text,
                "sentiment_signals": sentiment_signals,
                "sentiment_mix": context.aggregated_signals["sentiment_mix"]
            }
        
        # [5] get_routing_context
        elif tool_name == "get_routing_context":
            text = params.get("text")
            context = self.analyze_text(text)
            
            routing_signals = [
                {
                    "term": m.term,
                    "type": m.entry.type,
                    "target_entity": m.entry.target_entity,
                    "fandom_scope": m.entry.fandom_scope
                }
                for m in context.matches
            ]
            
            return {
                "tool": "get_routing_context",
                "text": text,
                "routing_signals": routing_signals,
                "risk_flags": context.aggregated_signals["risk_flags"]
            }
        
        # [6] get_causality_context
        elif tool_name == "get_causality_context":
            text = params.get("text")
            context = self.analyze_text(text)
            
            causality_signals = [
                {
                    "term": m.term,
                    "normalized_form": m.entry.normalized_form,
                    "trigger_type": m.entry.trigger_type,
                    "action_strength": m.entry.action_strength
                }
                for m in context.matches
            ]
            
            return {
                "tool": "get_causality_context",
                "text": text,
                "causality_signals": causality_signals,
                "action_triggers": context.aggregated_signals["action_triggers"]
            }
        
        # [7] get_playbook_context
        elif tool_name == "get_playbook_context":
            text = params.get("text")
            context = self.analyze_text(text)
            
            action_signals = [
                {
                    "term": m.term,
                    "type": m.entry.type,
                    "action_strength": m.entry.action_strength,
                    "risk_flag": m.entry.risk_flag
                }
                for m in context.matches
            ]
            
            return {
                "tool": "get_playbook_context",
                "text": text,
                "action_signals": action_signals,
                "risk_flags": context.aggregated_signals["risk_flags"],
                "target_entities": context.aggregated_signals["target_entities"]
            }
        
        else:
            raise ValueError(f"Unknown tool: {tool_name}")
    
    def create_mcp_tools(self) -> List[Dict[str, Any]]:
        """MCP ë„êµ¬ ì •ì˜ (inputSchema í†µì¼)"""
        return [
            {
                "name": "lookup_term",
                "description": "ë‹¨ì¼ ìš©ì–´ ì¡°íšŒ",
                "inputSchema": {  # â† inputSchemaë¡œ í†µì¼
                    "type": "object",
                    "properties": {"term": {"type": "string"}},
                    "required": ["term"]
                }
            },
            {
                "name": "lookup_terms",
                "description": "ë‹¤ì¤‘ ìš©ì–´ ì¡°íšŒ",
                "inputSchema": {
                    "type": "object",
                    "properties": {"terms": {"type": "array", "items": {"type": "string"}}},
                    "required": ["terms"]
                }
            },
            {
                "name": "analyze_text",
                "description": "í…ìŠ¤íŠ¸ ë¶„ì„ ë° íŒ¬ë¤ í‘œí˜„ ì¶”ì¶œ",
                "inputSchema": {
                    "type": "object",
                    "properties": {"text": {"type": "string"}},
                    "required": ["text"]
                }
            },
            {
                "name": "get_sentiment_context",
                "description": "SentimentAgentìš© ì»¨í…ìŠ¤íŠ¸",
                "inputSchema": {
                    "type": "object",
                    "properties": {"text": {"type": "string"}},
                    "required": ["text"]
                }
            },
            {
                "name": "get_routing_context",
                "description": "RouterAgentìš© ì»¨í…ìŠ¤íŠ¸",
                "inputSchema": {
                    "type": "object",
                    "properties": {"text": {"type": "string"}},
                    "required": ["text"]
                }
            },
            {
                "name": "get_causality_context",
                "description": "CausalityAgentìš© ì»¨í…ìŠ¤íŠ¸",
                "inputSchema": {
                    "type": "object",
                    "properties": {"text": {"type": "string"}},
                    "required": ["text"]
                }
            },
            {
                "name": "get_playbook_context",
                "description": "PlaybookAgentìš© ì»¨í…ìŠ¤íŠ¸",
                "inputSchema": {
                    "type": "object",
                    "properties": {"text": {"type": "string"}},
                    "required": ["text"]
                }
            }
        ]


