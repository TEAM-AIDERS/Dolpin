"""
Mock í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Stub êµ¬í˜„ìœ¼ë¡œ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
    python tests/test_mcp_mock.py
    ë˜ëŠ”
    pytest tests/test_mcp_mock.py -v
"""

import json
import sys
from pathlib import Path
from datetime import datetime, timezone

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.dolpin_langgraph.graph import compile_workflow
from src.dolpin_langgraph.nodes import router1_node
from src.dolpin_langgraph.state import AnalysisState, SpikeEvent, Message


def create_mock_spike_event() -> SpikeEvent:
    """Mock SpikeEvent ìƒì„±"""
    return {
        "keyword": "ë‰´ì§„ìŠ¤",
        "spike_rate": 3.5,
        "baseline": 100,
        "current_volume": 350,
        "detected_at": "2026-01-10T10:00:00Z",
        "time_window": "1h",
        "messages": [
            {
                "id": "550e8400-e29b-41d4-a716-446655440001",
                "source_message_id": "187654321",
                "text": "ë‰´ì§„ìŠ¤ ì»´ë°± ëŒ€ë°•!",
                "timestamp": "2026-01-10T10:00:00Z",
                "source": "twitter",
                "author_id": "user_123",
                "metrics": {"likes": 100, "retweets": 50},
                "is_anonymized": False,
                "detected_language": "ko"
            },
            {
                "id": "550e8400-e29b-41d4-a716-446655440002",
                "source_message_id": "187654322",
                "text": "ë‰´ì§„ìŠ¤ ìµœê³ ì•¼ ã… ã… ",
                "timestamp": "2026-01-10T10:05:00Z",
                "source": "twitter",
                "author_id": "user_456",
                "metrics": {"likes": 200, "retweets": 80},
                "is_anonymized": False,
                "detected_language": "ko"
            }
        ],
        "raw_kafka_message_ids": [
            "550e8400-e29b-41d4-a716-446655440000"
        ]
    }


def create_initial_state(spike_event: SpikeEvent, trace_id: str = "test-trace-001") -> AnalysisState:
    """ì´ˆê¸° State ìƒì„±"""
    return {
        # ì…ë ¥
        "spike_event": spike_event,
        
        # ë©”íƒ€ë°ì´í„°
        "trace_id": trace_id,
        "workflow_start_time": datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),
        
        # Router ê²°ì • (ì´ˆê¸°ê°’ None)
        "route1_decision": None,
        "route2_decision": None,
        "route3_decision": None,
        "positive_viral_detected": None,
        
        # ê° ì—ì´ì „íŠ¸ ê²°ê³¼ (ì´ˆê¸°ê°’ None)
        "spike_analysis": None,
        "sentiment_result": None,
        "causality_result": None,
        "legal_risk": None,
        "amplification_summary": None,
        "playbook": None,
        
        # ë…¸ë“œ ì¸ì‚¬ì´íŠ¸
        "node_insights": {},
        
        # ìµœì¢… ì¶œë ¥
        "executive_brief": None,
        
        # ì—ëŸ¬ ë¡œê·¸
        "error_logs": [],
        
        # Skip ì²˜ë¦¬
        "skipped": False,
        "skip_reason": None
    }


def print_state_summary(state: AnalysisState):
    """State ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š DOLPIN ë¶„ì„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê²°ê³¼")
    print("="*80)
    
    # ë©”íƒ€ë°ì´í„°
    print(f"\nğŸ” Trace ID: {state['trace_id']}")
    print(f"â±ï¸  ì‹œì‘ ì‹œê°„: {state['workflow_start_time']}")
    
    # Router ê²°ì •
    print(f"\nğŸ”€ Router ê²°ì •:")
    print(f"   Router 1ì°¨: {state.get('route1_decision')}")
    print(f"   Router 2ì°¨: {state.get('route2_decision')}")
    print(f"   Router 3ì°¨: {state.get('route3_decision')}")
    print(f"   ê¸ì • ë°”ì´ëŸ´: {state.get('positive_viral_detected')}")
    
    # Skip ì—¬ë¶€
    if state.get("skipped"):
        print(f"\nâ­ï¸  Skip: {state['skip_reason']}")
        return
    
    # ê° ë…¸ë“œ ì¸ì‚¬ì´íŠ¸
    if state.get("node_insights"):
        print(f"\nğŸ’¡ ë…¸ë“œ ì¸ì‚¬ì´íŠ¸:")
        for node, insight in state["node_insights"].items():
            print(f"   â€¢ {node}: {insight}")
    
    # ìµœì¢… ë¸Œë¦¬í•‘
    if state.get("executive_brief"):
        brief = state["executive_brief"]
        print(f"\nğŸ“‹ ì„ì› ë¸Œë¦¬í•‘:")
        print(f"   ìš”ì•½: {brief['summary']}")
        print(f"   ì‹¬ê°ë„: {brief['severity_score']}/10")
        print(f"   íŠ¸ë Œë“œ: {brief['trend_direction']}")
        print(f"   ê·¹ì„±: {brief['issue_polarity']}")
        print(f"   ì†Œìš” ì‹œê°„: {brief['analysis_duration_seconds']}ì´ˆ")
        
        if brief.get("spike_summary"):
            print(f"\n   ğŸ“ˆ Spike: {brief['spike_summary']}")
        if brief.get("sentiment_summary"):
            print(f"   ğŸ˜Š Sentiment: {brief['sentiment_summary']}")
        if brief.get("legal_summary"):
            print(f"   âš–ï¸  Legal: {brief['legal_summary']}")
        if brief.get("action_summary"):
            print(f"   ğŸ¯ Action: {brief['action_summary']}")
        if brief.get("opportunity_summary"):
            print(f"   ğŸŒŸ Opportunity: {brief['opportunity_summary']}")
        
        print(f"\n   ë¶„ì„ ìƒíƒœ:")
        for agent, status in brief["analysis_status"].items():
            emoji = "âœ…" if status == "success" else "â­ï¸" if status == "skipped" else "âŒ"
            print(f"      {emoji} {agent}: {status}")
        
        if brief.get("user_message"):
            print(f"\n   âš ï¸  ì‚¬ìš©ì ë©”ì‹œì§€: {brief['user_message']}")
    
    # ì—ëŸ¬ ë¡œê·¸
    if state.get("error_logs"):
        print(f"\nâŒ ì—ëŸ¬ ë¡œê·¸ ({len(state['error_logs'])}ê±´):")
        for error in state["error_logs"]:
            print(f"   â€¢ [{error['stage']}] {error['error_type']}: {error['message']}")
    
    print("\n" + "="*80 + "\n")


def save_result_to_file(state: AnalysisState, filename: str = "mock_result.json"):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    # tests/outputs ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(__file__).parent / "outputs"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # íŒŒì¼ ê²½ë¡œ
    filepath = output_dir / filename
    
    # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    def convert_datetime(obj):
        if isinstance(obj, datetime):
            return obj.isoformat().replace('+00:00', 'Z')
        return obj
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2, default=convert_datetime)
    
    print(f"ğŸ’¾ ê²°ê³¼ê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def run_test_case_1_positive_viral():
    """
    í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ê¸ì • ë°”ì´ëŸ´ (Amplification ê²½ë¡œ)
    
    ì¡°ê±´:
    - spike_rate: 3.5 (high)
    - spike_nature: "positive"
    - support: 0.6 (>= 0.5)
    - actionability_score: 0.7 (>= 0.6)
    
    ì˜ˆìƒ ê²½ë¡œ:
    SpikeAnalyzer â†’ Router1 (analyze) â†’ Sentiment â†’ Router2 (full_analysis) 
    â†’ Causality â†’ Router3 (amplification) â†’ Amplification â†’ Playbook â†’ ExecBrief
    
    TODO: ì‹¤ì œ Agent êµ¬í˜„ ì‹œ Stub ë°ì´í„°ë¥¼ ì‹¤ì œ ë¶„ì„ ê²°ê³¼ë¡œ êµì²´
    """
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ê¸ì • ë°”ì´ëŸ´ (Amplification ê²½ë¡œ)")
    print("-" * 80)
    
    spike_event = create_mock_spike_event()
    initial_state = create_initial_state(spike_event, trace_id="test-trace-001")
    
    # ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ë° ì‹¤í–‰
    app = compile_workflow()
    final_state = app.invoke(initial_state)
    
    # ê²°ê³¼ ì¶œë ¥
    print_state_summary(final_state)
    
    # ê²€ì¦
    assert final_state["route1_decision"] == "analyze", "Router 1ì°¨ê°€ analyzeì—¬ì•¼ í•¨"
    assert final_state["route2_decision"] == "full_analysis", "Router 2ì°¨ê°€ full_analysisì—¬ì•¼ í•¨"
    assert final_state["route3_decision"] == "amplification", "Router 3ì°¨ê°€ amplificationì´ì–´ì•¼ í•¨"
    assert final_state["positive_viral_detected"] == True, "ê¸ì • ë°”ì´ëŸ´ ê°ì§€ë˜ì–´ì•¼ í•¨"
    
    print("âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1 í†µê³¼!")
    
    return final_state


def run_test_case_2_skip():
    """
    í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: Skip ê²½ë¡œ
    
    ì¡°ê±´:
    - is_significant: False (ê¸‰ë“± ë¯¸ë‹¬)
    
    ì˜ˆìƒ ê²½ë¡œ:
    SpikeAnalyzer â†’ Router1 (skip) â†’ ì¢…ë£Œ
    
    TODO: ì‹¤ì œ SpikeAnalyzer êµ¬í˜„ ì‹œ spike_rate ê¸°ë°˜ìœ¼ë¡œ is_significant íŒë‹¨
    """
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: Skip ê²½ë¡œ")
    print("-" * 80)
    
    spike_event = create_mock_spike_event()
    spike_event["spike_rate"] = 1.1  # ë‚®ì€ ê¸‰ë“±ë¥ 
    
    initial_state = create_initial_state(spike_event, trace_id="test-trace-002")
    
    # TODO: í˜„ì¬ Stubì€ í•­ìƒ is_significant=True
    # ì‹¤ì œ êµ¬í˜„ ì‹œ spike_rate ê¸°ë°˜ íŒë‹¨ í•„ìš”
    print("âš ï¸  í˜„ì¬ Stubì€ í•­ìƒ is_significant=Trueë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    print("   ì‹¤ì œ SpikeAnalyzer êµ¬í˜„ ì‹œ spike_rate ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨í•  ì˜ˆì •ì…ë‹ˆë‹¤.")
    
    # ìˆ˜ë™ìœ¼ë¡œ skip ìƒíƒœ ì„¤ì • (í…ŒìŠ¤íŠ¸ìš©)
    initial_state["spike_analysis"] = {
        "is_significant": False,
        "spike_rate": 1.1,
        "spike_type": "organic",
        "spike_nature": "neutral",
        "peak_timestamp": "2026-01-10T10:30:00Z",
        "duration_minutes": 30,
        "confidence": 0.9,
        "actionability_score": 0.1,
        "data_completeness": "confirmed",
        "partial_data_warning": None,
        "viral_indicators": {
            "is_trending": False,
            "has_breakout": False,
            "max_rise_rate": "+10%",
            "breakout_queries": [],
            "cross_platform": ["twitter"],
            "international_reach": 0.0
        }
    }
    
    # Router1ë§Œ ì‹¤í–‰
    from src.dolpin_langgraph.nodes import router1_node
    final_state = router1_node(initial_state)
    
    print_state_summary(final_state)
    
    assert final_state["route1_decision"] == "skip", "Router 1ì°¨ê°€ skipì´ì–´ì•¼ í•¨"
    assert final_state["skipped"] == True, "skipped í”Œë˜ê·¸ê°€ Trueì—¬ì•¼ í•¨"
    
    print("âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2 í†µê³¼!")
    
    return final_state


def run_test_case_3_sentiment_only():
    """
    í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: Sentiment Only ê²½ë¡œ (ë‚®ì€ actionability)
    
    ì¡°ê±´:
    - actionability_score: 0.2 (< 0.3)
    - ë‹¨ìˆœ íŒ¬ ë°˜ì‘, ìœ„ê¸°/ê¸°íšŒ ì‹ í˜¸ ì—†ìŒ
    
    ì˜ˆìƒ ê²½ë¡œ:
    SpikeAnalyzer â†’ Router1 (analyze) â†’ Sentiment â†’ Router2 (sentiment_only) 
    â†’ Playbook â†’ ExecBrief
    
    âœ… nodes.pyëŠ” ìˆ˜ì •í•˜ì§€ ì•Šê³ ,
    í…ŒìŠ¤íŠ¸ ì½”ë“œì—ì„œ stateë¥¼ ì§ì ‘ ì„¸íŒ…í•˜ì—¬ ë¶„ê¸° ì¡°ê±´ì„ ë§Œë“ ë‹¤.
    """
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: Sentiment Only ê²½ë¡œ (ë‚®ì€ actionability)")
    print("-" * 80)
    
    spike_event = create_mock_spike_event()
    spike_event["keyword"] = "ì—ìŠ¤íŒŒ ì˜ˆì˜ë‹¤"  # ë‹¨ìˆœ íŒ¬ ë°˜ì‘
    initial_state = create_initial_state(spike_event, trace_id="test-trace-003")
    
    # TODO: nodes.pyì˜ spike_analyzer_node Stub ìˆ˜ì • í•„ìš”
    # actionability_scoreë¥¼ 0.2ë¡œ ì„¤ì •
    initial_state["spike_analysis"] = {
        "is_significant": True,
        "spike_rate": 2.0,
        "spike_type": "organic",
        "spike_nature": "positive",
        "peak_timestamp": "2026-01-10T10:30:00Z",
        "duration_minutes": 30,
        "confidence": 0.9,
        "actionability_score": 0.2,  # í•µì‹¬: ë‚®ì€ actionability
        "data_completeness": "confirmed",
        "partial_data_warning": None,
        "viral_indicators": {
            "is_trending": False,
            "has_breakout": False,
            "max_rise_rate": "+20%",
            "breakout_queries": [],
            "cross_platform": ["twitter"],
            "international_reach": 0.1
        }
    }

    initial_state["sentiment_result"] = {
        "dominant_sentiment": "support",
        "sentiment_distribution": {
            "support": 0.8,
            "neutral": 0.2
        },
        "confidence": 0.85,
        "sentiment_shift": "stable",
        "analyzed_count": 10,
        "representative_messages": {
            "support": ["ì—ìŠ¤íŒŒ ì§„ì§œ ì˜ˆì˜ë‹¤ ã… ã… "]
        }
    }

    app = compile_workflow()
    final_state = app.invoke(initial_state)

    print_state_summary(final_state)
    
    assert final_state["route1_decision"] == "analyze", \
        "Router 1ì°¨ëŠ” analyzeì—¬ì•¼ í•¨"

    assert final_state["route2_decision"] == "sentiment_only", \
        "Router 2ì°¨ëŠ” sentiment_onlyì—¬ì•¼ í•¨"

    assert final_state["route3_decision"] is None, \
        "Router 3ì°¨ëŠ” ìˆ˜í–‰ë˜ì§€ ì•Šì•„ì•¼ í•¨"

    print("âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3 í†µê³¼!")

    return final_state

def run_test_case_4_legal_crisis():
    """
    í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: Legal ê²½ë¡œ (ìœ„ê¸° ì‹ í˜¸ - ë³´ì´ì½§)
    
    ì¡°ê±´:
    - actionability_score: 0.5 (ì¤‘ê°„)
    - boycott: 0.3 (>= 0.2) â†’ ìœ„ê¸° ì‹ í˜¸
    - spike_nature: "negative"
    
    ì˜ˆìƒ ê²½ë¡œ:
    SpikeAnalyzer â†’ Router1 (analyze) â†’ Sentiment â†’ Router2 (full_analysis)
    â†’ Causality â†’ Router3 (legal) â†’ Legal RAG â†’ Playbook â†’ ExecBrief
    
    âš ï¸ nodes.pyëŠ” ìˆ˜ì •í•˜ì§€ ì•ŠìŒ
    í…ŒìŠ¤íŠ¸ ì½”ë“œì—ì„œ spike_analysis / sentiment_resultë¥¼ overrideí•˜ì—¬
    Legal ê²½ë¡œë¥¼ ê°•ì œë¡œ ìœ ë„í•¨
    """
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: Legal ê²½ë¡œ (ìœ„ê¸° ì‹ í˜¸ - ë³´ì´ì½§)")
    print("-" * 80)
    
    spike_event = create_mock_spike_event()
    spike_event["keyword"] = "XX ë¶ˆë§¤"
    initial_state = create_initial_state(spike_event, trace_id="test-trace-004")
    
    # TODO: nodes.py Stub ìˆ˜ì • í•„ìš”
    # spike_analyzer_node: spike_nature="negative", actionability_score=0.5
    # sentiment_node: boycott=0.3
    initial_state["spike_analysis"] = {
        "is_significant": True,
        "spike_rate": 2.5,
        "spike_type": "organic",
        "spike_nature": "negative",   # ìœ„ê¸°
        "peak_timestamp": "2026-01-10T10:30:00Z",
        "duration_minutes": 60,
        "confidence": 0.9,
        "actionability_score": 0.5,   # ì¤‘ê°„ êµ¬ê°„
        "data_completeness": "confirmed",
        "partial_data_warning": None,
        "viral_indicators": {
            "is_trending": True,
            "has_breakout": False,
            "max_rise_rate": "+150%",
            "breakout_queries": [],
            "cross_platform": ["twitter"],
            "international_reach": 0.2
        }
    }

    initial_state["sentiment_result"] = {
        "dominant_sentiment": "boycott",
        "confidence": 0.8,
        "sentiment_distribution": {
            "boycott": 0.3,
            "support": 0.2,
            "neutral": 0.3,
            "disappointment": 0.2
        },
        "analyzed_count": 50,
        "sentiment_shift": "worsening",
        "has_mixed_sentiment": True,
        "representative_messages": {
            "boycott": ["ì´ë²ˆ í™œë™ì€ ë¶ˆë§¤í•œë‹¤"]
        }
    }

    # Router2 ì‹¤í–‰ (full_analysis ìœ ë„)
    from src.dolpin_langgraph.nodes import router2_node
    state_after_r2 = router2_node(initial_state)

    # Causality stub ì‹¤í–‰
    from src.dolpin_langgraph.nodes import causality_node
    state_after_causality = causality_node(state_after_r2)

    # Router3 ì‹¤í–‰ (Legal ìœ ë„)
    # positive_viral_detected = False ìƒíƒœ
    from src.dolpin_langgraph.nodes import router3_node
    final_state = router3_node(state_after_causality)

    print_state_summary(final_state)

    assert final_state["route2_decision"] == "full_analysis", \
        "Router 2ì°¨ê°€ full_analysisì—¬ì•¼ í•¨"

    assert final_state["route3_decision"] == "legal", \
        "Router 3ì°¨ê°€ legalì´ì–´ì•¼ í•¨"

    print("âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4 í†µê³¼!")

    return final_state

def run_test_case_5_legal_keyword():
    """
    í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5: Legal ê²½ë¡œ (ë²•ì  í‚¤ì›Œë“œ ê°ì§€)
    
    ì¡°ê±´:
    - dominant_sentiment: "meme_negative"
    - ëª…ì˜ˆí›¼ì† í‚¤ì›Œë“œ í¬í•¨
    - legal_risk: high_risk
    
    ì˜ˆìƒ ê²½ë¡œ:
    SpikeAnalyzer â†’ Router1 (analyze) â†’ Sentiment â†’ Router2 (full_analysis)
    â†’ Causality â†’ Router3 (legal) â†’ Legal RAG (high_risk) â†’ Playbook â†’ ExecBrief
    
    âš ï¸ nodes.py ìˆ˜ì • ì—†ì´ í…ŒìŠ¤íŠ¸ ì½”ë“œì—ì„œ override
    """
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5: Legal ê²½ë¡œ (ë²•ì  í‚¤ì›Œë“œ)")
    print("-" * 80)
    
    spike_event = create_mock_spike_event()
    spike_event["keyword"] = "XX ëª…ì˜ˆí›¼ì†"
    initial_state = create_initial_state(spike_event, trace_id="test-trace-005")
    
    # sentiment_node: dominant_sentiment="meme_negative"
    # legal_rag_node: clearance_status="high_risk"

    # SpikeAnalyzer ê²°ê³¼ ê°•ì œ ì„¸íŒ…
    initial_state["spike_analysis"] = {
        "is_significant": True,
        "spike_rate": 2.8,
        "spike_type": "organic",
        "spike_nature": "negative",
        "peak_timestamp": "2026-01-10T10:30:00Z",
        "duration_minutes": 45,
        "confidence": 0.9,
        "actionability_score": 0.6,  # full_analysis ìœ ë„
        "data_completeness": "confirmed",
        "partial_data_warning": None,
        "viral_indicators": {
            "is_trending": True,
            "has_breakout": False,
            "max_rise_rate": "+180%",
            "breakout_queries": [],
            "cross_platform": ["twitter"],
            "international_reach": 0.1
        }
    }

    # Sentiment ê²°ê³¼ ê°•ì œ ì„¸íŒ…
    # meme_negative â†’ ìœ„ê¸° ì‹ í˜¸
    initial_state["sentiment_result"] = {
        "dominant_sentiment": "meme_negative",
        "confidence": 0.75,
        "sentiment_distribution": {
            "meme_negative": 0.4,
            "support": 0.2,
            "neutral": 0.3,
            "disappointment": 0.1
        },
        "analyzed_count": 40,
        "sentiment_shift": "worsening",
        "has_mixed_sentiment": True,
        "representative_messages": {
            "meme_negative": ["ì´ê±´ ì§„ì§œ ëª…ì˜ˆí›¼ì†ì´ë‹¤"]
        }
    }

    # Router2 ì‹¤í–‰ â†’ full_analysis
    from src.dolpin_langgraph.nodes import router2_node
    state_after_r2 = router2_node(initial_state)

    # Causality ì‹¤í–‰ (stub)
    from src.dolpin_langgraph.nodes import causality_node
    state_after_causality = causality_node(state_after_r2)

    # Router3 ì‹¤í–‰ â†’ legal
    from src.dolpin_langgraph.nodes import router3_node
    state_after_r3 = router3_node(state_after_causality)

    # Legal RAG ê²°ê³¼ ê°•ì œ high_risk ì„¤ì •
    state_after_r3["legal_risk"] = {
        "overall_risk_level": "high",
        "clearance_status": "high_risk",
        "confidence": 0.95,
        "rag_required": True,
        "rag_performed": True,
        "rag_confidence": 0.9,
        "risk_assessment": "ëª…ì˜ˆí›¼ì† ì†Œì§€ ìˆìŒ",
        "recommended_action": ["ë²•ë¬´íŒ€ ê²€í†  ìš”ì²­"],
        "referenced_documents": ["í˜•ë²• ì œ307ì¡°"],
        "signals": {
            "legal_keywords_detected": True,
            "matched_keywords": ["ëª…ì˜ˆí›¼ì†"],
            "reason": "ë²•ì  í‚¤ì›Œë“œ ê°ì§€"
        }
    }

    final_state = state_after_r3

    print_state_summary(final_state)

    assert final_state["route2_decision"] == "full_analysis", \
        "Router 2ì°¨ê°€ full_analysisì—¬ì•¼ í•¨"

    assert final_state["route3_decision"] == "legal", \
        "Router 3ì°¨ê°€ legalì´ì–´ì•¼ í•¨"

    assert final_state["legal_risk"]["clearance_status"] == "high_risk", \
        "Legal RAG ê²°ê³¼ê°€ high_riskì—¬ì•¼ í•¨"

    print("âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5 í†µê³¼!")

    return final_state


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\n" + "="*80)
    print("ğŸš€ DOLPIN Mock ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*80)
    print("\nğŸ’¡ í˜„ì¬ ì‹¤í–‰ ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸: 1, 2")
    print("   TODO: í…ŒìŠ¤íŠ¸ 3, 4, 5ëŠ” nodes.py Stub ìˆ˜ì • í›„ ì‹¤í–‰ ê°€ëŠ¥\n")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ê¸ì • ë°”ì´ëŸ´
    result1 = run_test_case_1_positive_viral()
    if result1:
        save_result_to_file(result1, "mock_result_case1_positive_viral.json")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: Skip
    result2 = run_test_case_2_skip()
    if result2:
        save_result_to_file(result2, "mock_result_case2_skip.json")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: Sentiment Only
    run_test_case_3_sentiment_only()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: Legal - Crisis
    run_test_case_4_legal_crisis()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5: Legal - Keyword
    run_test_case_5_legal_keyword()
    
    print("\n" + "="*80)
    print("ğŸ‰ ëª¨ë“  Mock í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì™„ë£Œ! (5/5)")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()
